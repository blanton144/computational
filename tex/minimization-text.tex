These notes draw heavily on {\it Numerical Recipes}, a valuable
resource for entry-level understanding of numerical methods relevant
to physics.

\section{Minimization}

Optimization, either minimization or maximization of functions, is a
close relation of root-finding, but it has some fundamental
differences. There is a sense in which they are the same problem,
because finding a stationary point of $f(x)$ is like finding a root of
$\dd{f}/\dd{x}$. Nevertheless, they do differ (particularly in
multiple dimensions, due fundamentally to the fact that the gradient
of a function has special properties helpful to finding its minimum).

In the game of optimization, the most important categorization is
between {\it convex} and {\it non-convex} problems. Convex problems
are one with a single optimum --- e.g. a single minimum. So if you
find a minimum, you have found the minimum. Nonconvex problems can
have multiple optima --- i.e. more than one local minimum, only one of
which is a global minimum (unless there's a tie!).

We start with the simplest example of a convex problem, that of a
quadratic optimization. We've already seen this problem in the context
of SVD, but let's connect it explicitly to the idea of minimization.

\section{Minimization of quadratic functions}

If you have a one-dimensional parabola, it has exactly one
extreme. We will write the parabola as:
\begin{equation}
f(x) = \alpha x^2 + \beta x + \gamma
\end{equation}

\noindent {\bf In this case, what is the extremum of the function?}

\begin{answer}
You can easily find this extreme analytically of course:
leads to
\begin{eqnarray}
f'(x) =  2 \alpha x + \beta  &=& 0 \cr
x  &=& - \frac{\beta}{2\alpha} \cr
\end{eqnarray}
\end{answer}

Typically, we will be handed a function, not given an explicit set of
quadratic parameters (and we'll see later we actually want to
use the quadratic form as an approximation to more general
functions). 

\noindent {\bf How many function evaluations do I need to do to
  determine the parameters of the quadratic?}

\begin{answer}
Three, because there are three unknowns. If you choose points $a$,
$b$, and $c$, these evaluations give you the equations:
\begin{eqnarray}
\alpha a^2 + \beta a + \gamma &=& f(a) \cr
\alpha b^2 + \beta b + \gamma &=& f(b) \cr
\alpha c^2 + \beta c + \gamma &=& f(c) \cr
\end{eqnarray}
or:
\begin{equation}
  \left(\begin{array}{ccc}
    a^2 & a & 1 \cr
    b^2 & b & 1 \cr
    c^2 & c & 1 
  \end{array}\right) \cdot
  \left(\begin{array}{c}
    \alpha \cr
    \beta \cr
    \gamma
    \end{array}\right) = 
  \left(\begin{array}{c}
    f(a) \cr
    f(b) \cr
    f(c)
    \end{array}\right) = 
\end{equation}
If $a$, $b$, and $c$ are distinct, this matrix will be nonsingular.
\end{answer}

In fact, one can work through this matrix to find $\alpha$, $\beta$,
nd $\gamma$ (you don't really use $\gamma$, but you do need to
calculate it) and thus the minimum (this is the same as Brent's
method). Analytically you get the result:
\begin{equation}
  x = b - \frac{1}{2}
  \frac{(b-a)^2 \left[f(b) - f(c)\right] - (b-c)^2
  \left[f(b) - f(a)\right]}
  {(b-a) \left[f(b) - f(c)\right] - (b-c) \left[f(b) - f(a)\right]}
\end{equation}

This method works great if the function is actually quadratic (not
something else, and not linear). But is very rare for a function to be
known to be quadratic, and not have the ability to calculate its
derivative directly. The real use of this method is as a way to
iterate twoards a solution.

\section{Golden Section Search}

For functions about which you know nothing, there is a more fool-proof
method, akin to bisection, which is {\it golden section} search.

Imagine you start with a bracketing set of points $a$, $b$, and $c$,
such that $f(b)<f(a)$ and $f(b)<f(c)$. Define the fractional position
of $b$ as:
\begin{equation}
w = \frac{b-a}{c-a}
\end{equation}
Now we will pick a next trial point $x$. This will define some new
triplet, depending on whether it is higher or lower than $f(b)$. Let's
call the fraction position beyond $b$:
\begin{equation}
z = \frac{x-b}{c-a}
\end{equation}
If the new triplet is the first three points, it has fractional length
$w+z$. If it is the last three points, it has fractional length
$1-w$. It seems reasonable to want these to be the same:
\begin{equation}
w+z = 1 - w \quad\rightarrow\quad z = 1 - 2w
\end{equation}
which leaves each length as $1-w$. 

If you are applying this iteratively, the fractional position of $x$
within the new bracket should be the same as $b$ was in the old
bracket (since the latter was created by the same process):
\begin{equation}
\frac{z}{1-w} = w
\end{equation}
and then plugging in for $z$, and solving for $w$ we can find:
\begin{equation}
w = \frac{3-\sqrt{5}}{2} \approx 0.382
\end{equation}
This is the {\it golden section}.

This method will continually bracket the minimum further and further
down. 

\section{Brent's one-dimensional minimization}

A workable one-dimensional minimization routine combines these two
methods, and is called Brent's Method. Like the root-finding Brent, it
uses parabolic approximations, but it keeps track of a bracketing
interval, and under certain conditions it reverts to golden section
search.

These conditions are that:
\begin{itemize}
\item The parabolic step falls outside the  bracketing interval.
\item The parabolic step is greater than the step before last (!).
\end{itemize}

You shall, for your homework, implement this method.

\section{$N$-dimensional minimization}

Things get trickier in higher numbers of dimensions. A phenomenally
bad way to minimize in high dimensions is to iteratively minimize
along different coordinate directions.  This will be extremely slow
even in pretty reasonable cases.  It will get caught in some
degeneracy valleys.  Another bad way that at first sounds good is {\it
  steepest descent}, where you minimize in the gradient direction,
successively. It is easy to draw situations where most starting
points will lead to very slow convergence.

Modern methods are robust to this problem; at times you will hear
physicists describe this as if it were a mysterious, unassailable
problem, but computer science has had it solved in a huge number of
practical cases for decades.

The key idea for improving convergence is the same as that used in
parabolic minimization. Near the minimum of the function, it will be
quadratic at lowest order. Let's put the minimum at the origin for
simplicity. If you expand it in Taylor series:
\begin{equation}
  f(\vec{x}) = f(\vec{0})
  + \sum_i \left[\frac{\partial f}{\partial x_i}\right]_{\vec{0}}  x_i
  + \frac{1}{2} \sum_{ij} \left[\frac{\partial^2 f}{\partial x_i \partial x_j}\right]_{\vec{0}}
  x_i x_j
\end{equation}

\noindent {\bf What is the gradient of this Taylor expansion?}

\begin{answer}
\begin{equation}
\sum_i \frac{\partial f}{\partial x_i} {\hat e}_i = \sum_i
\left[\frac{\partial f}{\partial x_i}\right]_{\vec{0}} {\hat e}_i +
\sum_{ij} \left[\frac{\partial^2 f}{\partial x_i \partial
    x_j}\right]_{\vec{0}} x_j {\hat e}_i
\end{equation}
which can be written as:
\begin{equation}
\vec{\nabla}f = \left[\vec{\nabla}f \right]_{\vec{0}} +
\mat{A}\cdot{\vec{x}}
\end{equation}
where the {\it Hessian matrix} is:
\begin{equation}
A_{ij} = 
\left[\frac{\partial^2 f}{\partial x_i \partial x_j}\right]_{\vec{0}}
\end{equation}
Clearly we can solve this problem!  If you actually knew the Hessian
matrix you could leap to the bottom of a quadratic.
\end{answer}

\noindent {\bf If you had to choose successive directions to minimize
  one-dimensionally in, which would they be?}

\begin{answer}
The eigenvectors of $\mat{A}$. These eigenvectors will bring you
straight to the bottom of a quadratic, in $N$ line minimizations. In a
real situation, where the function was only approximately quadratic,
of course it would take some more iterations.

However, you can't do this in general, since usually you do not know
the form of the Hessian.
\end{answer}

There is one sort of case where you may know the Hessian matrix. That
is when your function $f$ is quadratic in the parameters. A classic
example is least squares minimization to a linear model (sometimes
called $\chi^2$ minimization, which is only appropriate to say if you
are fitting data to a model with Gaussian noise). In this case if you
have data points $g_i$, and a model $m_i = \sum_j a_j M_{ij}$ which is
a linear function of unknowns $a_j$, then the function you want to
minimize is:
\begin{equation}
f = \sum_i \left( g_i - \sum_{j} a_j M_{ij} \right)^2
\end{equation}
and your Hessian matrix can be constructed and the problem above
solved. Always remember that if your function is quadratic in its
parameters than minimizing it should be easy --- it is convex and you
can solve it in one matrix operation!

\section{Conjugate Gradient}

In more general cases, we do not know the Hessian matrix explicitly
and need to estimate its structure based on evaluations of $f$ (and
its derivatives). How to estimate the Hessian matrix efficiently with 
function and derivative evaluations is the subject of the {\it
  conjugate gradient} method and its variants. In general, you will
want to use one of the modern variants on conjugate gradient, which
are good for many low-enough-dimensional and smooth problems. A good,
extensive introduction is given by
\href{http://www.cs.cmu.edu/~jrs/jrspapers.html}{\color{red} Shewchuk
  (1994)}.

The idea of conjugate gradient is simple, but its implementation is
tricky. Basically, the idea is that if you knew the eigenvectors of
the Hessian matrix, you could successively minimize along its
eigenvectors and reach the minimum in $N$ steps. This is clear when
you remember that the eigenvectors form a basis in which the matrix is
diagonal. Thus, if you construct a rotation matrix \mat{R} from the
eigenvectors:
\begin{equation}
\mat{A} = \mat{R}^T \cdot \mat{D} \cdot \mat{R}
\end{equation}
where $\mat{D}$ is a diagonal matrix. Thus, any quadratic form can be
written in the eigenbasis ${\hat e}'$ as:
\begin{equation}
\sum_i a_i (x_i' - {\bar x}_i)^2
\end{equation}
that is, without any cross-terms between different
dimensions. Furthmore, we can define a more general transformation
\mat{T} so that:
\begin{equation}
\mat{A} = \mat{T}^T \cdot \mat{I} \cdot \mat{T}
\end{equation}
just by renormalizing the eigenvectors. It is also worth noting at
this stage that the columns of \mat{T} satisfy:
\begin{equation}
\vec{t}_i \cdot\mat{A}\cdot \vec{t}_j = \delta_{ij}
\end{equation}
If I transform into a new space defined by \mat{T} then we can start
in {\it any} direction and do steepest descent.

This fact is of limited use on its own. After all, you still need to
determine the Hessian matrix and construct the eigenbasis. The magic
of the conjugate gradient algorithm is that you can generate searches
along the relevant directions. You start in any direction
$\vec{h_0}=\vec{g_0}$ and line minimize in that direction. Then you
update according to:
\begin{eqnarray}
\vec{g}_{i+1} &=& \vec{g}_i - \lambda_i \mat{A}\cdot\vec{h}_i \cr
\vec{h}_{i+1} &=& \vec{g}_{i+1} + \gamma_i \vec{h}_i 
\end{eqnarray}
with:
\begin{eqnarray}
\lambda_i &=& \frac{\vec{g}_i \cdot
  \vec{h}_i}{\vec{h}_i\cdot\mat{A}\cdot\vec{h}_j} \cr
\gamma_i &=& \frac{\vec{g}_{i+1} \cdot
  \vec{g}_{i+1}}{\vec{g}_i\cdot\vec{g}_i}
\end{eqnarray}
Given this updating scheme, you line minimize successively on
$\vec{h}_i$. 

The directions $\vec{h}$ and $\vec{g}$ have the magical
property that for $j<i$, $\vec{g}_i$ is orthogonal to all the previous
$\vec{g}_j$ and $\vec{h}_j$, and crucially:
\begin{equation}
\vec{h}_i \cdot \mat{A} \cdot \vec{h}_j = 0 
\end{equation}
so the resulting directions have the property we want. Showing that
this is the case takes more time than I have here. 

Except the $\lambda_i$ still require the knowledge of
$\mat{A}$. Luckily we can use the following trick. If $\vec{g}_i = -
\nabla f(\vec{P}_i)$, then line minimize and pick the next
$\vec{g}_{i+1}$ as the gradient again, this is the same as the update
for $\vec{g}$ above. It does not require $\mat{A}$.

We can see this because in this case:
\begin{equation}
\vec{g}_i = - \mat{A}\cdot\vec{P}_i + \vec{b}
\end{equation}
so:
\begin{equation}
\vec{g}_{i+1} = - \mat{A}\cdot (\vec{P}_i + \lambda \vec{h}_i) + \vec{b}
\end{equation}
and so:
\begin{equation}
\vec{g}_{i+1} = \vec{g}_i - \lambda \mat{A}\cdot\vec{h}_i
\end{equation}
with $\lambda$ chosen at the line minimum. This $\lambda$ turns out is
the same as $\lambda_i$ used above. To show this, note that at the
minimum:
\begin{equation}
  \vec{h}_i\cdot \nabla f = -\vec{h}_i \cdot \vec{g}_{i+1}  = 0
\end{equation}
Then you can solve for $\lambda$ in the above equation and you get the
value for $\lambda_i$ above.

This version of the algorithm is known as the Fletcher-Reeves
algorithm. You start your minimization along the gradient, then you
update according to how $\vec{h}_i$ evolves under the iteration
above. If your function is quadratic, you converge in $N$ steps. If
not, you still proceed quite rapidly to the minimum.

You need derivatives! In multi-dimensions, it is worthwhile to simply
use finite difference derivatives.

\section{Quasi-Newtonian Methods}

There are other techniques for finding the minimum through generating
the Hessian matrix information, specifically building up a model for
its inverse. These are known as quasi-newtonian methods. The most
commonly used of these nowadays is called
Broyden-Fletcher-Goldfarb-Shanno (BFGS).  This is the method of choice
for minimization problems with a moderate number of dimensions.
