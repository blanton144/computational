\section{Basic idea of integration}

Integration of functions is an important calculation in computational
physics, both as a fundamental task and as a component of larger
problems. Many integrals do not have closed forms and require
numerical computation.

\noindent {\bf What is the definition of an integral?}

\begin{answer}
An integral is defined by the limit:
\begin{equation}
\int_a^b {\rm d}x f(x) = \lim_{{\rm d}x \rightarrow 0} \left[{\rm d}x
  \sum_{i=1}^{(b-a)/{\rm d}x} f(x_i) \right]
\end{equation}
where $x_i$ are spaced between $a$ and $b$ with separations ${\rm
  d}x$.
\end{answer}

\noindent {\bf What is a simple numerical estimate of an integral?}

\begin{answer}
Just to perform this sum with some finite ${\rm d}x$:
\begin{equation}
\int_a^b {\rm d}x f(x) = \left[{\rm d}x \sum_{i=1}^{(b-a)/{\rm d}x}
  f(x_i) \right]
\end{equation}
where $x_i$ are spaced between $a$ and $b$ with separations ${\rm
  d}x$.
\end{answer}

This is just a particular case of the more general form that most
integration methods take, which is that it can be approximated as some
linear combination of evaluations of the function:
\begin{equation}
  \int_a^b {\rm d}x f(x) = \sum_{i=1}^N 
  f(x_i) w_i
\end{equation}

\section{Trapezoid rule}

The simple estimate above can be thought of as approximating the
function as piecewise constant. Obviously there are better
approximations that can be made! Better algorithms for integration
generally boil down to better models of the function. In this respect,
integration is closely allied to interpolation of functions.

The trapezoid rule is the result of integrating a linear interpolation
of the function. Each term in the integral will become:
\begin{equation}
\frac{1}{2} {\rm d}x \left( f_i + f_{i+1} \right) 
\end{equation}
The next term is:
\begin{equation}
\frac{1}{2} {\rm d}x \left( f_{i+1} + f_{i+2} \right) 
\end{equation}
For equally spaced points, then $w_i = {\rm d}x$, except for $w_1=
w_{N} = {\rm d}x/2$.

\noindent {\bf For what sort of function is the trapezoid rule exactly
  correct?} 

\begin{answer}
For a linear function. Of course, this property is not very useful!!
\end{answer}

\section{Simpson's rule}

Simpson's rule represents the next level of sophistication in
interpolation. Here, the function is approximated locally around the
points $i-1$, $i$, $i+1$, as a quadratic:
\begin{equation}
f(x) = \alpha' + \beta' x + \gamma' x^2
\end{equation}
This is not a very convenient form. Let us instead use:
\begin{equation}
  f(x) = \alpha + \beta \left(\frac{x - x_i}{{\rm d}x}\right) +
  \gamma \left(\frac{x - x_i}{{\rm d}x}\right)^2 = 
  \alpha + \beta y
  + \gamma y^2
\end{equation}
with a change of variable to $y = (x-x_i)/{\rm d}x$.  For a set of
three points, $i-1$, $i$, and $i+1$, you can fit the parabola using
the fact:
\begin{eqnarray}
f_{i-1} &=& \alpha - \beta + \gamma \cr
f_{i} &=& \alpha \cr
f_{i+1} &=& \alpha + \beta + \gamma
\end{eqnarray}
This can be easily solved:
\begin{eqnarray}
\alpha &=& f_i \cr
\gamma &=& \frac{f_{i+1}+f_{i-1}}{2} - f_i \cr
\beta &=& \frac{f_{i+1}-f_{i-1}}{2}
\end{eqnarray}

\noindent {\bf What is the integral over the region defined by these
  three points?}

\begin{answer}
The integral over the region defined by these three points :
\begin{eqnarray}
  \int_{x_{i-1}}^{x_{i+1}} {\rm d}x f(x) &=& {\rm d}x \int_{-1}^{1} {\rm
    d}y \left(\alpha + \beta y + \gamma y^2\right) \cr
  &=& {\rm d}x \left[\alpha y + \frac{\beta}{2} y^2 + \frac{\gamma}{3}
    y^3\right]_{-1}^{1}  \cr
  &=& {\rm d}x \left[2 \alpha + \frac{2\gamma}{3}\right]
\end{eqnarray}
Plugging in $\alpha$ and $\gamma$:
\begin{equation}
  \int_{x_{i-1}}^{x_{i+1}} {\rm d}x f(x)
  = {\rm d}x \left(2 f_i +
  \frac{f_{i+1} + f_{i-1}}{3} - \frac{2}{3} f_i\right) 
  = {\rm d}x \left(\frac{1}{3} f_{i-1}
  + \frac{4}{3} f_i 
  + \frac{1}{3} f_{i+1}\right)
\end{equation}
\end{answer}

Simpson's rule comes from using this approximation across the length
from $a$ to $b$, by dividing the interval into an even number of
segments, and integrating each separately. This yields a full
summation:
\begin{equation}
  \int_a^b {\rm d}x f(x) = \sum_{i=1}^N = {\rm d}x \left[\frac{1}{3}
    f_1 + \frac{4}{3} f_2 + \frac{2}{3} f_3 + \frac{4}{3} f_4 + \ldots
    + \frac{2}{3} f_{N-2} + \frac{4}{3} f_{N-1} + \frac{1}{3} f_N
    \right]
\end{equation}

Because this is applied to two segments at a time, it requires an even
number of segments, which means $N$ must be odd.

\noindent {\bf The weights for the three points used in in Simpson's
  rule are set to exactly integral a quadratic function --- a second
  degree polynomial. What must $N$ be to exactly integrate an
  $M$-degree polynomial?}

\begin{answer}
$N = M + 1$. We can show this as follows. Each of the $N$ points $x_k$
  yields a linear equality:
\begin{equation}
\label{eq:weights}
 f_k = \sum_{j=0}^M \alpha_j x^j
\end{equation}
that can be used to determine the coefficients of the function, and
thus its integral.  So this yields a system of $N$ linear equations,
with $M+1$ unknowns. So to guarantee a solution, you need $N=M+1$.
\end{answer}

These methods are good methods, but it turns out we can be even
cleverer. But before we do so, we have a little bit of work to do.

\section{Rescaling of integrals}

It may appear trivial, but just as in differentiation, there are
rescaling of integrals that can be performed for various reasons of
convenience or otherwise. 

The simplest rescaling is linear, which just rescales the limits of
the integral:
\begin{equation}
 I = \int_a^b {\rm d}x f(x) = \frac{b-a}{b'-a'} \int_{a'}^{b'} {\rm d}x' f(x(x'))
\end{equation}
which simply follows from the tranformation:
\begin{eqnarray}
x' &=& (x-a) \left(\frac{b' - a'}{b-a}\right) + a' \cr
{\rm d}x' &=& {\rm d}x \left(\frac{b' - a'}{b-a}\right)
\end{eqnarray}
or:
\begin{eqnarray}
x &=& (x'-a') \left(\frac{b - a}{b'-a'}\right) + a' \cr
{\rm d}x &=& {\rm d}x' \left(\frac{b - a}{b'-a'}\right)
\end{eqnarray}

This is a pretty trival rescaling, but it can be useful if you can
rescale an integral to a previously calculated integral. We will use
this below in the specific case: $a'=-1$, $b'=1$:
\begin{equation}
 I = \int_a^b {\rm d}x f(x) = \frac{b-a}{2} \int_{-1}^{1} {\rm d}x' f(x(x'))
\end{equation}
This will allow us to develop some useful algorithms for the specific
range $-1$ to $1$, which can then be generalized to any finite range.

If we want to alter $[-1, 1]$ to an infinite range that is possible
too.  For example:
\begin{eqnarray}
x &=& a \frac{1+x'}{1-x'}  \cr
dx &=& a \left(\frac{1}{1-x'} + \frac{1+x'}{(1-x')^2}\right) \cr
&=& a {\rm d}x' \frac{1 -x' + 1 + x'}{(1-x')^2}  \cr
&=& {\rm d}x' \frac{2a}{(1-x')^2}  \cr
\end{eqnarray}
which lets us rewrite an infinite range:
\begin{equation}
 I = \int_0^\infty {\rm d}x f(x) = \int_{-1}^{1} {\rm d}x' \frac{2a
   x'}{(1-x')^2} f(x(x'))
\end{equation}
In this case, $a$ is a choice to be made, and $x=a$ when $x'=0$. So
there are better choices for $a$ than others -- you want it to be
somewhere near where the integral is expected to reach about half its
total.

The other forms of weighting are given in the book, and may be derived
similarly. 

\section{Gaussian quadrature}

Now we have all the tools to derive one of the workhorse algorithms
for integrating function, which is Gaussian quadrature. Gaussian
quadrature has the advantage that it yields a systematic way to write
an algorithm for integration which utilizes $N$ points, that is {\it
  exact} for any polynomial of order $2N-1$ or less. Note that this is
much better than we found before, the path we were on for Simpson's
rule, which utilized $N+1$ points to exactly integrate a polynomial of
$N$ points. It turns out that the improvement is gained by choosing
the points carefully.

We will show how to do this for the integral:
\begin{equation}
\int_{-1}^{1} {\rm d}x f(x)
\end{equation}
where $f(x)$ is a $2N-1$ degree polynomial (or less).  Clearly we can
rescale the limits as necessary above for the problem at hand.

We are seeking an exact formula for the integral of this function
which is:
\begin{equation}
\int_{-1}^{1} {\rm d}x f(x) = \sum_{i=1}^N w_i f(x_i)
\end{equation}

The derivation of this is neat. Note that the derivation in the book
is extremely confusing and contains at least one error.

We will use the Legendre polynomials to aid us. In fact, it will be
the roots of the Legendre polynomials (where they are zero) that turn
out to be the locations of the integration points.

\noindent {\bf What are the Legendre Polynomials? Where have you seen
  them before.}

\begin{answer}
  They usually arise in the physics curriculum because they are the
  solutions to Laplace's Equation ($\nabla^2 \Phi =0$) under
  cylindrical symmetry. They also have some interesting properties. We
  will refer to them here as $P_n(x)$, where $-1 < x < 1$, and $n$ is
  the order of the Legendre Polynomial. They have the property that
  each Legendre Polynomial is a polynomial of order $n$:

  \begin{equation}
    P_n(x) = \sum_{i=0}^n a_n x^n 
  \end{equation}

  Specifically, they are:
  \begin{equation}
    P_n = \frac{1}{2^n n!} \frac{{\rm d}^n(x^2 -1)^n}{{\rm d} x^n}
  \end{equation}

  Or:
  \begin{eqnarray}
    P_0(x) &=& 1 \cr
    P_1(x) &=& x \cr
    P_2(x) &=& \frac{1}{2}(3x^2 -1) \cr
    P_3(x) &=& \frac{1}{2}(5x^3 - 3x) \cr
    P_4(x) &=& \frac{1}{8}(35^4 - 30x^2 + 3) \cr
    &\ldots& 
  \end{eqnarray}

  They form a complete set in function space (any function can be
  expressed as a sum of a sufficient number of Legendre Polynomials).
  A related property is that any polynomial of order $n$ can be
  expressed as a sum of Legendre Polynomials with orders $\le n$.
\end{answer}

\noindent {\bf All the Legendre Polynomials are orthogonal to each
  other. What does that mean?}

\begin{answer}
  It means that their dot products are zero. Functions live in a linear
  vector space. E.g. it is infinite-dimensional, and one set of basis
  functions are Dirac $\delta$-functions. You can define a dot product
  in that space as:
  \begin{equation}
    q(x) \cdot r(x) = \int_{-1}^{1} {\rm d}x q(x) r(x)
  \end{equation}
  As we will see below, this choice is not unique!

  In any case, it means that the statement that Legendre Polynomials
  are orthogonal means the following:
  \begin{equation}
    \int_{-1}^{1} {\rm d}x P_n(x) P_m(x) = \delta_{nm}
    \frac{2}{2n+1}
  \end{equation}
\end{answer}

\noindent {\bf If all the Legendre Polynomials are orthogonal to each
  other, and any polynomial of order $n$ can be expressed as a sum of
  Legendre Polynomials with order $\le n$, then what is this
  integral:}
  
  \begin{equation}
    \int_{-1}^{1} {\rm d}x P_{n+1}(x) x^m 
  \end{equation}
  
  \begin{answer}
    0
  \end{answer}

We start by noting that $f(x)$  can be in general factored in the
following way:
\begin{equation}
f(x) = q(x) P_N(x) + r(x)
\end{equation}
where we choose $q(x)$ to be an $N-1$-degree polynomial. The first
term is therefore a polynomial of order $2N-1$, or less. Since we can
choose the coefficients of the $q(x)$ polynomial to be whatever we
want, we can always match the coefficients of all the polynomial terms
of order $N$ or greater in $f(x)$. This leaves a remainder $r(x)$
which is an $N-1$-degree polynomial (or less).

The integral:
\begin{equation}
\int_{-1}^{1} {\rm d}x q(x) P_N(x) = 0
\end{equation}
because the Legendre polynomials are always orthogonal to lower-order
polynomials.

So:
\begin{equation}
\int_{-1}^{1} {\rm d}x f(x) = \int_{-1}^{1} {\rm d}x r(x)
\end{equation}
Since $r(x)$ is an $N-1$-degree polynomial or less, there is a way to
integrate the function with $N$ points or less, as we found above.

The locations of these points can be found as follows. The integral is
now known to be writable as:
\begin{equation}
\int_{-1}^{1} {\rm d}x f(x) = \sum_{i=1}^{N} w_i f(x_i) =
\sum_{i+1}^{N} w_i q(x_i) P_N(x_i) + 
\sum_{i+1}^{N} w_i r(x_i)
\end{equation}
If we choose the points $x_i$ to be the $N$ roots of the Legendre
polynomial of order $N$, then:
\begin{equation}
\int_{-1}^{1} {\rm d}x f(x) = \sum_{i=1}^{N} w_i r(x_i)
\end{equation}
where:
\begin{equation}
r(x) = \sum_{j=0}^{N-1} \alpha_j x_j
\end{equation}
This is great, and we also know how to set the $w_i$. These are
derived in the same way as for Simpson's rule.  Using the appropriate
linear set of equations analogous to Equation \ref{eq:weights}, you
can express the solution for the coefficients $\alpha_j$ in terms of
values of the function $f(x_i)$, which by design is equal to $r(x_i)$
at the chosen points, and then given the closed form of the integral
of the polynomial, this translates into a form for $w_i$.

The notebook shows an implementation given the weights and locations
determined for $N=4$, and demonstrates performance up to $2N-1$.

Rather than determine weights and locations yourself to higher order,
the SciPy routines in its {\tt integrate} module already contain this
information. In particular, {\tt fixed\_quad} performs the fixed-order
Gaussian quadrature that we find here.

\section{Generalizations of Gaussian quadrature}

The Gaussian quadrature method is good for smooth functions. However,
if it is not smooth, or in particular has singularities, it will be an
issue. In addition, there turns out to be plenty of scope in
generalizing the method to handle certain non-polynomial functions
exactly.

Specifically, it turns out that you can generally find {\it exact}
expressions for integrals of the following form:
\begin{equation}
\int_{-1}^{1} {\rm d}x W(x) f(x) 
\end{equation}
where $W(x)$ is a known function and $f(x)$ is a polynomial. 

The proof involves redefining the dot product between two functions
$q(x)$ and $r(x)$:
\begin{equation}
  q(x) \cdot r(x) = \int_{-1}^{1} {\rm d}x W(x) q(x) r(x)
\end{equation}
Then it turns out we can find a complete basis set of polynomals that
are orthogonal under this definition. Legendre Polynomials are just
one case of this, for $W(x)=1$. The locations are determined by the
roots of these new polynomials, and the weights are determined in an
analogous manner.

As an example, look at the problem:
\begin{equation}
\int_{-1}^{1} {\rm d}x \frac{1}{\sqrt{1-x^2}} = \pi
\end{equation}
If we use regular Gaussian quadrature, our answers are very bad.

But we can define $W(x)=1/\sqrt{1-x^2}$ and $f(x)=1$. This is called
{\it Gauss-Chebyshev} quadrature. SciPy doesn't have this directly,
but it does have a routine that gives you the weights.  This works
very well for integrating over this singularity.

Another generally useful form has $W(x) = \exp(-x^2)$. This yields
Gauss-Hermite polynomials, and Gauss-Hermite quadrature. A slightly
altered Gaussian is a common form for a number of real-world
distributions. 

\section{Adaptive quadrature}

\section{Multidimensional integrals}
